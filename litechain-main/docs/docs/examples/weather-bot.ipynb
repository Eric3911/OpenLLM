{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ec042b3",
   "metadata": {},
   "source": [
    "# Simple Bot with Weather Tool\n",
    "\n",
    "Below is a code example of a bot you can talk too which has the ability of checking the weather, it has memory, it is using OpenAI functions, and it streams its outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d41fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Literal, TypedDict\n",
    "from litechain import debug\n",
    "\n",
    "from litechain.contrib.llms.open_ai import (\n",
    "    OpenAIChatChain,\n",
    "    OpenAIChatDelta,\n",
    "    OpenAIChatMessage,\n",
    ")\n",
    "\n",
    "\n",
    "class Memory(TypedDict):\n",
    "    history: List[OpenAIChatMessage]\n",
    "\n",
    "\n",
    "memory = Memory(history=[])\n",
    "\n",
    "\n",
    "class WeatherReturn(TypedDict):\n",
    "    location: str\n",
    "    forecast: str\n",
    "    temperature: str\n",
    "\n",
    "\n",
    "def save_message_to_memory(message: OpenAIChatMessage) -> OpenAIChatMessage:\n",
    "    memory[\"history\"].append(message)\n",
    "    return message\n",
    "\n",
    "\n",
    "def update_delta_on_memory(delta: OpenAIChatDelta) -> OpenAIChatDelta:\n",
    "    if memory[\"history\"][-1].role != delta.role and delta.role is not None:\n",
    "        memory[\"history\"].append(\n",
    "            OpenAIChatMessage(role=delta.role, content=delta.content, name=delta.name)\n",
    "        )\n",
    "    else:\n",
    "        memory[\"history\"][-1].content += delta.content\n",
    "    return delta\n",
    "\n",
    "\n",
    "def weather_bot(user_input: str):\n",
    "    def reply_with_current_weather(\n",
    "        location: str, format: Literal[\"celsius\", \"fahrenheit\"] = \"celsius\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Gets the current weather in a given location, use this function for any questions related to the weather\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        location\n",
    "            The city to get the weather, e.g. San Francisco. Guess the location from user messages\n",
    "\n",
    "        format\n",
    "            A string with the full content of what the given role said\n",
    "        \"\"\"\n",
    "\n",
    "        result = WeatherReturn(\n",
    "            location=location,\n",
    "            forecast=\"sunny\",\n",
    "            temperature=\"25 C\" if format == \"celsius\" else \"77 F\",\n",
    "        )\n",
    "\n",
    "        save_message_to_memory(\n",
    "            OpenAIChatMessage(\n",
    "                role=\"function\",\n",
    "                content=json.dumps(result),\n",
    "                name=\"get_current_weather\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return weather_reply_chain(result)\n",
    "\n",
    "    weather_chain = debug(\n",
    "        OpenAIChatChain[str, OpenAIChatDelta](\n",
    "            \"WeatherChain\",\n",
    "            lambda user_input: [\n",
    "                *memory[\"history\"],\n",
    "                save_message_to_memory(\n",
    "                    OpenAIChatMessage(role=\"user\", content=user_input),\n",
    "                ),\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            functions=[reply_with_current_weather],\n",
    "            temperature=0,\n",
    "        )\n",
    "    ).map(update_delta_on_memory)\n",
    "\n",
    "    weather_reply_chain = OpenAIChatChain[WeatherReturn, OpenAIChatDelta](\n",
    "        \"WeatherReplyChain\",\n",
    "        lambda weather: [\n",
    "            OpenAIChatMessage(role=\"user\", content=user_input),\n",
    "            OpenAIChatMessage(\n",
    "                role=\"user\",\n",
    "                content=f\"Output from the weather system: {json.dumps(weather)}\",\n",
    "            ),\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    return weather_chain(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "731c90c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32m> WeatherChain\u001b[39m\n",
      "\n",
      "\u001b[33mAssistant:\u001b[39m Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "from litechain.utils.chain import collect_final_output\n",
    "\n",
    "_ = await collect_final_output(weather_bot(\"hi there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a44fb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[32m> WeatherChain\u001b[39m\n",
      "\n",
      "\u001b[33mFunction:\u001b[39m reply_with_current_weather(location='Amsterdam')\n",
      "\n",
      "\u001b[32m> WeatherReplyChain\u001b[39m\n",
      "\n",
      "\u001b[33mAssistant:\u001b[39m Yes, it is hot today in Amsterdam with a temperature of 25 degrees Celsius and sunny weather."
     ]
    }
   ],
   "source": [
    "_ = await collect_final_output(weather_bot(\"is it hot today in Amsterdam?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79f25374",
   "metadata": {},
   "source": [
    "The bot is working well, it replies chit-chat messages as well as calling the weather function when needed, and replying to the user in natural language.\n",
    "\n",
    "Let's inspect what's inside the bot memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a13c103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OpenAIChatMessage(role='user', content='hi there', name=None),\n",
       " OpenAIChatMessage(role='assistant', content='Hello! How can I assist you today?', name=None),\n",
       " OpenAIChatMessage(role='user', content='is it hot today in Amsterdam?', name=None),\n",
       " OpenAIChatMessage(role='function', content='{\"location\": \"Amsterdam\", \"forecast\": \"sunny\", \"temperature\": \"25 C\"}', name='get_current_weather'),\n",
       " OpenAIChatMessage(role='assistant', content='Yes, it is hot today in Amsterdam with a temperature of 25 degrees Celsius and sunny weather.', name=None)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory['history']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "862136fe",
   "metadata": {},
   "source": [
    "It saved both the conversation and the results of the function call, this way, continued conversations will be able to use the previous context, include the previous function result.\n",
    "\n",
    "That's it, if you have any questions about this example, [join our discord community](https://discord.gg/48ZM5KkKgw) and we can help you out.\n",
    "\n",
    "Also, if you are interested in running a bot like this inside a nice UI, check out our [docs on Chainlit](../ui/chainlit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82d862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
